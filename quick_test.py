"""
ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
ì ì€ ì—í”¼ì†Œë“œë¡œ ë¹ ë¥´ê²Œ ë™ì‘ í™•ì¸
"""

import numpy as np
import torch
from traffic_env import TrafficEnvironment, FixedTimeController
from dqn_agent import DQNAgent, DoubleDQNAgent
import json
import os


def quick_test():
    """ë¹ ë¥¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("ğŸš¦ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("="*60)
    
    # í™˜ê²½ ë° ì—ì´ì „íŠ¸ ìƒì„±
    env = TrafficEnvironment()
    env.set_scenario('normal')
    
    STATE_DIM = 7
    ACTION_DIM = 2
    
    agent = DQNAgent(
        state_dim=STATE_DIM,
        action_dim=ACTION_DIM,
        learning_rate=0.001,
        gamma=0.95,
        epsilon_start=1.0,
        epsilon_end=0.01,
        epsilon_decay=0.995,
        buffer_capacity=1000,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‘ê²Œ
        batch_size=32,
        target_update_freq=10
    )
    
    print(f"ë””ë°”ì´ìŠ¤: {agent.device}")
    print(f"ìƒíƒœ ì°¨ì›: {STATE_DIM}")
    print(f"í–‰ë™ ì°¨ì›: {ACTION_DIM}")
    
    # ì§§ì€ í•™ìŠµ (10 ì—í”¼ì†Œë“œ)
    print("\nğŸ“š í•™ìŠµ ì‹œì‘ (10 ì—í”¼ì†Œë“œ)")
    
    history = {
        'episode_rewards': [],
        'avg_waiting_times': [],
        'losses': []
    }
    
    for episode in range(10):
        state = env.reset()
        episode_reward = 0
        episode_loss = 0
        step_count = 0
        
        done = False
        while not done:
            action = agent.select_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            
            agent.store_transition(state, action, reward, next_state, done)
            loss = agent.update()
            
            episode_reward += reward
            episode_loss += loss
            step_count += 1
            state = next_state
        
        agent.decay_epsilon()
        
        history['episode_rewards'].append(episode_reward)
        history['avg_waiting_times'].append(info['avg_waiting_time'])
        history['losses'].append(episode_loss / step_count if step_count > 0 else 0)
        
        print(f"Episode {episode+1}/10 - Reward: {episode_reward:.2f}, "
              f"ëŒ€ê¸°ì‹œê°„: {info['avg_waiting_time']:.2f}ì´ˆ, "
              f"Îµ: {agent.epsilon:.4f}")
    
    # í‰ê°€ (5 ì—í”¼ì†Œë“œ)
    print("\nğŸ“Š í‰ê°€ ì‹œì‘ (5 ì—í”¼ì†Œë“œ)")
    
    eval_results = {
        'episode_rewards': [],
        'avg_waiting_times': []
    }
    
    for episode in range(5):
        state = env.reset()
        episode_reward = 0
        
        done = False
        while not done:
            action = agent.select_action(state, training=False)
            next_state, reward, done, info = env.step(action)
            episode_reward += reward
            state = next_state
        
        eval_results['episode_rewards'].append(episode_reward)
        eval_results['avg_waiting_times'].append(info['avg_waiting_time'])
        
        print(f"Eval {episode+1}/5 - Reward: {episode_reward:.2f}, "
              f"ëŒ€ê¸°ì‹œê°„: {info['avg_waiting_time']:.2f}ì´ˆ")
    
    print(f"\ní‰ê·  í‰ê°€ Reward: {np.mean(eval_results['episode_rewards']):.2f}")
    print(f"í‰ê·  ëŒ€ê¸°ì‹œê°„: {np.mean(eval_results['avg_waiting_times']):.2f}ì´ˆ")
    
    # Baseline ë¹„êµ
    print("\nğŸ”§ Baseline (ê³ ì • ì£¼ê¸° 30ì´ˆ) í‰ê°€ (5 ì—í”¼ì†Œë“œ)")
    
    controller = FixedTimeController(cycle_time=30)
    baseline_results = {
        'episode_rewards': [],
        'avg_waiting_times': []
    }
    
    for episode in range(5):
        state = env.reset()
        controller.reset()
        episode_reward = 0
        
        done = False
        while not done:
            action = controller.get_action(state)
            next_state, reward, done, info = env.step(action)
            episode_reward += reward
            state = next_state
        
        baseline_results['episode_rewards'].append(episode_reward)
        baseline_results['avg_waiting_times'].append(info['avg_waiting_time'])
    
    print(f"Baseline í‰ê·  Reward: {np.mean(baseline_results['episode_rewards']):.2f}")
    print(f"Baseline í‰ê·  ëŒ€ê¸°ì‹œê°„: {np.mean(baseline_results['avg_waiting_times']):.2f}ì´ˆ")
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs('./results', exist_ok=True)
    
    test_results = {
        'training': history,
        'evaluation': eval_results,
        'baseline': baseline_results
    }
    
    with open('./results/quick_test_results.json', 'w') as f:
        json.dump(test_results, f, indent=4)
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ê²°ê³¼ ì €ì¥: ./results/quick_test_results.json")
    
    print("\n" + "="*60)
    print("ğŸ‰ ëª¨ë“  ê¸°ëŠ¥ì´ ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤!")
    print("="*60)


if __name__ == "__main__":
    quick_test()