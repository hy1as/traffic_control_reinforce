"""
í†µí•© ì‹¤í—˜ ì›Œí¬í”Œë¡œìš°
1ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
2ë‹¨ê³„: ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµ ì‹¤í—˜
"""

import numpy as np
import torch
import json
import os
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List
from traffic_env import TrafficEnvironment, FixedTimeController
from dqn_agent import DQNAgent, DoubleDQNAgent


def find_best_hyperparameters(tuning_results_path: str) -> Dict:
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ì—ì„œ ìµœì  íŒŒë¼ë¯¸í„° ì°¾ê¸°
    
    Args:
        tuning_results_path: íŠœë‹ ê²°ê³¼ JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬
    """
    print("\n" + "="*60)
    print("ğŸ” ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„ íƒ")
    print("="*60)
    
    with open(tuning_results_path, 'r') as f:
        results = json.load(f)
    
    # ê° ì‹¤í—˜ì—ì„œ ìµœì ê°’ ì°¾ê¸°
    best_params = {
        'state_dim': 7,
        'action_dim': 2
    }
    
    # Learning Rateì—ì„œ ìµœê³  ì„±ëŠ¥
    lr_results = results['Learning_Rate']
    best_lr = max(lr_results.items(), key=lambda x: x[1]['avg_reward'])
    best_params['learning_rate'] = float(best_lr[0].split('_')[1])
    print(f"âœ… ìµœì  Learning Rate: {best_params['learning_rate']}")
    print(f"   í‰ê·  Reward: {best_lr[1]['avg_reward']:.2f}")
    print(f"   í‘œì¤€í¸ì°¨: {best_lr[1]['std_reward']:.2f}")
    
    # Discount Factorì—ì„œ ìµœê³  ì„±ëŠ¥
    gamma_results = results['Discount_Factor']
    best_gamma = max(gamma_results.items(), key=lambda x: x[1]['avg_reward'])
    best_params['gamma'] = float(best_gamma[0].split('_')[1])
    print(f"\nâœ… ìµœì  Discount Factor: {best_params['gamma']}")
    print(f"   í‰ê·  Reward: {best_gamma[1]['avg_reward']:.2f}")
    print(f"   í‘œì¤€í¸ì°¨: {best_gamma[1]['std_reward']:.2f}")
    
    # Batch Sizeì—ì„œ ìµœê³  ì„±ëŠ¥
    batch_results = results['Batch_Size']
    best_batch = max(batch_results.items(), key=lambda x: x[1]['avg_reward'])
    best_params['batch_size'] = int(best_batch[0].split('_')[1])
    print(f"\nâœ… ìµœì  Batch Size: {best_params['batch_size']}")
    print(f"   í‰ê·  Reward: {best_batch[1]['avg_reward']:.2f}")
    print(f"   í‘œì¤€í¸ì°¨: {best_batch[1]['std_reward']:.2f}")
    
    # Buffer Sizeì—ì„œ ìµœê³  ì„±ëŠ¥
    buffer_results = results['Buffer_Size']
    best_buffer = max(buffer_results.items(), key=lambda x: x[1]['avg_reward'])
    best_params['buffer_capacity'] = int(best_buffer[0].split('_')[1])
    print(f"\nâœ… ìµœì  Buffer Size: {best_params['buffer_capacity']}")
    print(f"   í‰ê·  Reward: {best_buffer[1]['avg_reward']:.2f}")
    print(f"   í‘œì¤€í¸ì°¨: {best_buffer[1]['std_reward']:.2f}")
    
    # ê¸°íƒ€ ê³ ì • íŒŒë¼ë¯¸í„°
    best_params['epsilon_start'] = 1.0
    best_params['epsilon_end'] = 0.01
    best_params['epsilon_decay'] = 0.995
    best_params['target_update_freq'] = 100
    
    print("\n" + "="*60)
    print("ğŸ“‹ ìµœì¢… ì„ íƒëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°")
    print("="*60)
    for key, value in best_params.items():
        print(f"{key:25s}: {value}")
    print("="*60)
    
    return best_params


class ExperimentLogger:
    """ì‹¤í—˜ ë¡œê·¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, log_path: str = './results/experiment_log.txt'):
        self.log_path = log_path
        self.start_time = datetime.now()
        os.makedirs(os.path.dirname(log_path) if os.path.dirname(log_path) else '.', exist_ok=True)
        
        # ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
        with open(self.log_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("ê°•í™”í•™ìŠµ êµí†µ ì‹ í˜¸ë“± ì œì–´ ì‹¤í—˜ ë¡œê·¸\n")
            f.write("="*80 + "\n")
            f.write(f"ì‹¤í—˜ ì‹œì‘ ì‹œê°„: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
    
    def log(self, message: str, level: str = "INFO"):
        """ë¡œê·¸ ë©”ì‹œì§€ ê¸°ë¡"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] [{level}] {message}\n"
        
        with open(self.log_path, 'a', encoding='utf-8') as f:
            f.write(log_entry)
        
        # ì½˜ì†”ì—ë„ ì¶œë ¥
        print(log_entry.strip())
    
    def log_section(self, title: str):
        """ì„¹ì…˜ í—¤ë” ê¸°ë¡"""
        with open(self.log_path, 'a', encoding='utf-8') as f:
            f.write("\n" + "="*80 + "\n")
            f.write(f"{title}\n")
            f.write("="*80 + "\n")
        print(f"\n{'='*80}\n{title}\n{'='*80}")
    
    def log_hyperparameters(self, params: Dict):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡"""
        with open(self.log_path, 'a', encoding='utf-8') as f:
            f.write("\n[í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •]\n")
            f.write("-"*80 + "\n")
            for key, value in params.items():
                f.write(f"  {key:25s}: {value}\n")
            f.write("-"*80 + "\n\n")
    
    def log_scenario_start(self, scenario: str, algorithm: str, num_episodes: int):
        """ì‹œë‚˜ë¦¬ì˜¤ í•™ìŠµ ì‹œì‘ ê¸°ë¡"""
        self.log(f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario}' - {algorithm} í•™ìŠµ ì‹œì‘ (ì—í”¼ì†Œë“œ: {num_episodes})")
    
    def log_scenario_result(self, scenario: str, algorithm: str, results: Dict):
        """ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ê¸°ë¡"""
        with open(self.log_path, 'a', encoding='utf-8') as f:
            f.write(f"\n[ì‹œë‚˜ë¦¬ì˜¤: {scenario} - {algorithm} ê²°ê³¼]\n")
            f.write("-"*80 + "\n")
            f.write(f"  í‰ê·  Reward: {np.mean(results.get('episode_rewards', [])):.2f}\n")
            f.write(f"  í‰ê·  ëŒ€ê¸°ì‹œê°„: {np.mean(results.get('avg_waiting_times', [])):.2f}ì´ˆ\n")
            f.write(f"  í‰ê·  ì²˜ë¦¬ ì°¨ëŸ‰: {np.mean(results.get('total_vehicles_passed', [])):.1f}ëŒ€\n")
            if 'max_queue_lengths' in results:
                f.write(f"  í‰ê·  ìµœëŒ€ ëŒ€ê¸°: {np.mean(results.get('max_queue_lengths', [])):.1f}ëŒ€\n")
            f.write("-"*80 + "\n\n")
    
    def log_training_progress(self, scenario: str, algorithm: str, episode: int, 
                             total_episodes: int, avg_reward: float, avg_waiting: float, epsilon: float):
        """í•™ìŠµ ì§„í–‰ ìƒí™© ê¸°ë¡"""
        if episode % 500 == 0 or episode == total_episodes - 1:
            self.log(f"{scenario} - {algorithm}: Episode {episode+1}/{total_episodes} | "
                    f"Reward: {avg_reward:.2f} | ëŒ€ê¸°ì‹œê°„: {avg_waiting:.2f}ì´ˆ | Îµ: {epsilon:.4f}")
    
    def log_final_summary(self, summary_results: Dict):
        """ìµœì¢… ìš”ì•½ ê¸°ë¡"""
        with open(self.log_path, 'a', encoding='utf-8') as f:
            f.write("\n" + "="*80 + "\n")
            f.write("ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½\n")
            f.write("="*80 + "\n\n")
            
            f.write(f"í•˜ì´í¼íŒŒë¼ë¯¸í„° ì†ŒìŠ¤: {summary_results.get('params_source', 'unknown').upper()}\n")
            if 'hyperparameters' in summary_results:
                f.write("\n[ì‚¬ìš©ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°]\n")
                for key, value in summary_results['hyperparameters'].items():
                    f.write(f"  {key:25s}: {value}\n")
            
            f.write("\n[ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¹„êµ]\n")
            f.write("-"*80 + "\n")
            f.write(f"{'ì‹œë‚˜ë¦¬ì˜¤':<20} {'ì•Œê³ ë¦¬ì¦˜':<20} {'í‰ê·  ëŒ€ê¸°ì‹œê°„':>15} {'í‰ê·  Reward':>15} {'ê°œì„ ìœ¨':>12}\n")
            f.write("-"*80 + "\n")
            
            for scenario, data in summary_results['scenarios'].items():
                baseline_wait = data['baseline']['avg_waiting_time']
                for algo in ['dqn', 'ddqn', 'baseline']:
                    wait_time = data[algo]['avg_waiting_time']
                    reward = data[algo]['avg_reward']
                    improvement = ((baseline_wait - wait_time) / baseline_wait * 100) if algo != 'baseline' else 0.0
                    
                    algo_name = {
                        'dqn': 'DQN',
                        'ddqn': 'Double DQN',
                        'baseline': 'Baseline'
                    }[algo]
                    
                    f.write(f"{scenario:<20} {algo_name:<20} {wait_time:>15.2f}ì´ˆ {reward:>15.2f} {improvement:>11.2f}%\n")
            
            f.write("-"*80 + "\n")
    
    def log_graph_generation(self, graph_name: str):
        """ê·¸ë˜í”„ ìƒì„± ê¸°ë¡"""
        self.log(f"ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: {graph_name}")
    
    def log_completion(self):
        """ì‹¤í—˜ ì™„ë£Œ ê¸°ë¡"""
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        with open(self.log_path, 'a', encoding='utf-8') as f:
            f.write("\n" + "="*80 + "\n")
            f.write("ì‹¤í—˜ ì™„ë£Œ\n")
            f.write("="*80 + "\n")
            f.write(f"ì‹œì‘ ì‹œê°„: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ì´ ì†Œìš” ì‹œê°„: {duration}\n")
            f.write("="*80 + "\n")
        
        self.log(f"ì‹¤í—˜ ì™„ë£Œ! ì´ ì†Œìš” ì‹œê°„: {duration}")


def load_default_hyperparameters() -> Dict:
    """
    í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ê°€ ì—†ì„ ë•Œ ê¸°ë³¸ê°’ ì‚¬ìš©
    
    Returns:
        ê¸°ë³¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
    """
    print("\nâš ï¸  í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("   ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n")
    
    return {
        'state_dim': 7,
        'action_dim': 2,
        'learning_rate': 0.001,
        'gamma': 0.95,
        'batch_size': 64,
        'buffer_capacity': 10000,
        'epsilon_start': 1.0,
        'epsilon_end': 0.01,
        'epsilon_decay': 0.995,
        'target_update_freq': 100
    }


def train_agent_with_params(
    agent,
    env: TrafficEnvironment,
    num_episodes: int,
    scenario: str,
    save_dir: str,
    logger: ExperimentLogger = None
) -> Dict:
    """ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ì—ì´ì „íŠ¸ í•™ìŠµ"""
    from tqdm import tqdm
    
    os.makedirs(save_dir, exist_ok=True)
    env.set_scenario(scenario)
    
    history = {
        'episode_rewards': [],
        'episode_lengths': [],
        'avg_waiting_times': [],
        'total_vehicles_passed': [],
        'losses': [],
        'epsilons': [],
        'max_queue_lengths': []
    }
    
    algorithm_name = "DQN" if isinstance(agent, DQNAgent) and not isinstance(agent, DoubleDQNAgent) else "Double DQN"
    
    print(f"\nğŸš¦ í•™ìŠµ ì‹œì‘: {scenario} ì‹œë‚˜ë¦¬ì˜¤")
    print(f"   ì´ ì—í”¼ì†Œë“œ: {num_episodes}")
    
    for episode in tqdm(range(num_episodes), desc=f"Training {scenario}"):
        state = env.reset()
        episode_reward = 0
        episode_loss = 0
        step_count = 0
        
        done = False
        while not done:
            action = agent.select_action(state, training=True)
            next_state, reward, done, info = env.step(action)
            agent.store_transition(state, action, reward, next_state, done)
            loss = agent.update()
            
            episode_reward += reward
            episode_loss += loss
            step_count += 1
            state = next_state
        
        agent.decay_epsilon()
        
        history['episode_rewards'].append(episode_reward)
        history['episode_lengths'].append(step_count)
        history['avg_waiting_times'].append(info['avg_waiting_time'])
        history['total_vehicles_passed'].append(info['total_vehicles_passed'])
        history['losses'].append(episode_loss / step_count if step_count > 0 else 0)
        history['epsilons'].append(agent.epsilon)
        history['max_queue_lengths'].append(info['max_queue_length'])
        
        # ì£¼ê¸°ì  ì¶œë ¥ ë° ë¡œê·¸
        if (episode + 1) % 500 == 0:
            recent_rewards = history['episode_rewards'][-100:]
            recent_waiting = history['avg_waiting_times'][-100:]
            print(f"\n   Episode {episode+1}/{num_episodes}")
            print(f"   í‰ê·  Reward: {np.mean(recent_rewards):.2f}")
            print(f"   í‰ê·  ëŒ€ê¸°ì‹œê°„: {np.mean(recent_waiting):.2f}ì´ˆ")
            print(f"   Îµ: {agent.epsilon:.4f}")
            
            if logger:
                logger.log_training_progress(
                    scenario, algorithm_name, episode, num_episodes,
                    np.mean(recent_rewards), np.mean(recent_waiting), agent.epsilon
                )
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_path = os.path.join(save_dir, f'agent_{scenario}_optimized.pt')
    agent.save(final_path)
    print(f"âœ… ëª¨ë¸ ì €ì¥: {final_path}")
    if logger:
        logger.log(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_path}")
    
    return history


def evaluate_agent_optimized(
    agent,
    env: TrafficEnvironment,
    num_episodes: int,
    scenario: str
) -> Dict:
    """ìµœì í™”ëœ ì—ì´ì „íŠ¸ í‰ê°€"""
    env.set_scenario(scenario)
    
    results = {
        'episode_rewards': [],
        'avg_waiting_times': [],
        'total_vehicles_passed': [],
        'max_queue_lengths': []
    }
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        
        done = False
        while not done:
            action = agent.select_action(state, training=False)
            next_state, reward, done, info = env.step(action)
            episode_reward += reward
            state = next_state
        
        results['episode_rewards'].append(episode_reward)
        results['avg_waiting_times'].append(info['avg_waiting_time'])
        results['total_vehicles_passed'].append(info['total_vehicles_passed'])
        results['max_queue_lengths'].append(info['max_queue_length'])
    
    return results


def plot_training_curves(history: Dict, save_path: str = None, window_size: int = 100):
    """í•™ìŠµ ê³¡ì„  ì‹œê°í™” (ì´ë™ í‰ê· ì„  í¬í•¨)"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    episodes = np.arange(1, len(history['episode_rewards']) + 1)
    
    # ì´ë™ í‰ê·  ê³„ì‚° í•¨ìˆ˜
    def moving_average(data, window):
        return np.convolve(data, np.ones(window)/window, mode='valid')
    
    # Episode Rewards
    axes[0, 0].plot(episodes, history['episode_rewards'], alpha=0.3, color='blue', label='Raw')
    if len(history['episode_rewards']) >= window_size:
        ma_rewards = moving_average(history['episode_rewards'], window_size)
        ma_episodes = np.arange(window_size, len(history['episode_rewards']) + 1)
        axes[0, 0].plot(ma_episodes, ma_rewards, color='blue', linewidth=2, label=f'MA({window_size})')
    axes[0, 0].set_title('ì—í”¼ì†Œë“œë³„ Reward', fontsize=12, fontweight='bold')
    axes[0, 0].set_xlabel('ì—í”¼ì†Œë“œ')
    axes[0, 0].set_ylabel('Total Reward')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # Average Waiting Time
    axes[0, 1].plot(episodes, history['avg_waiting_times'], alpha=0.3, color='red', label='Raw')
    if len(history['avg_waiting_times']) >= window_size:
        ma_waiting = moving_average(history['avg_waiting_times'], window_size)
        ma_episodes = np.arange(window_size, len(history['avg_waiting_times']) + 1)
        axes[0, 1].plot(ma_episodes, ma_waiting, color='red', linewidth=2, label=f'MA({window_size})')
    axes[0, 1].set_title('ì—í”¼ì†Œë“œë³„ í‰ê·  ëŒ€ê¸°ì‹œê°„', fontsize=12, fontweight='bold')
    axes[0, 1].set_xlabel('ì—í”¼ì†Œë“œ')
    axes[0, 1].set_ylabel('ëŒ€ê¸°ì‹œê°„ (ì´ˆ)')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Loss
    axes[1, 0].plot(episodes, history['losses'], alpha=0.3, color='green', label='Raw')
    if len(history['losses']) >= window_size:
        ma_losses = moving_average(history['losses'], window_size)
        ma_episodes = np.arange(window_size, len(history['losses']) + 1)
        axes[1, 0].plot(ma_episodes, ma_losses, color='green', linewidth=2, label=f'MA({window_size})')
    axes[1, 0].set_title('í•™ìŠµ Loss', fontsize=12, fontweight='bold')
    axes[1, 0].set_xlabel('ì—í”¼ì†Œë“œ')
    axes[1, 0].set_ylabel('Loss')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Epsilon
    axes[1, 1].plot(episodes, history['epsilons'], color='purple', linewidth=2)
    axes[1, 1].set_title('íƒí—˜ë¥  (Îµ)', fontsize=12, fontweight='bold')
    axes[1, 1].set_xlabel('ì—í”¼ì†Œë“œ')
    axes[1, 1].set_ylabel('Epsilon')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    plt.close()


def plot_scenario_comparison(summary_results: Dict, save_path: str = None):
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ ê·¸ë˜í”„"""
    scenarios = list(summary_results['scenarios'].keys())
    algorithms = ['dqn', 'ddqn', 'baseline']
    algo_labels = ['DQN', 'Double DQN', 'Baseline']
    colors = ['#2E86AB', '#A23B72', '#F18F01']
    
    # ë°ì´í„° ì¤€ë¹„
    waiting_times = {algo: [] for algo in algorithms}
    rewards = {algo: [] for algo in algorithms}
    
    for scenario in scenarios:
        for algo in algorithms:
            waiting_times[algo].append(summary_results['scenarios'][scenario][algo]['avg_waiting_time'])
            rewards[algo].append(summary_results['scenarios'][scenario][algo]['avg_reward'])
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    x = np.arange(len(scenarios))
    width = 0.25
    
    # ëŒ€ê¸°ì‹œê°„ ë¹„êµ
    for i, (algo, label) in enumerate(zip(algorithms, algo_labels)):
        axes[0].bar(x + i*width, waiting_times[algo], width, label=label, color=colors[i], alpha=0.8)
    
    axes[0].set_xlabel('ì‹œë‚˜ë¦¬ì˜¤', fontsize=12)
    axes[0].set_ylabel('í‰ê·  ëŒ€ê¸°ì‹œê°„ (ì´ˆ)', fontsize=12)
    axes[0].set_title('ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ëŒ€ê¸°ì‹œê°„ ë¹„êµ', fontsize=14, fontweight='bold')
    axes[0].set_xticks(x + width)
    axes[0].set_xticklabels([s.replace('_', ' ').title() for s in scenarios], rotation=15, ha='right')
    axes[0].legend(fontsize=10)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # Reward ë¹„êµ
    for i, (algo, label) in enumerate(zip(algorithms, algo_labels)):
        axes[1].bar(x + i*width, rewards[algo], width, label=label, color=colors[i], alpha=0.8)
    
    axes[1].set_xlabel('ì‹œë‚˜ë¦¬ì˜¤', fontsize=12)
    axes[1].set_ylabel('í‰ê·  Reward', fontsize=12)
    axes[1].set_title('ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  Reward ë¹„êµ', fontsize=14, fontweight='bold')
    axes[1].set_xticks(x + width)
    axes[1].set_xticklabels([s.replace('_', ' ').title() for s in scenarios], rotation=15, ha='right')
    axes[1].legend(fontsize=10)
    axes[1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    plt.close()


def plot_improvement_comparison(summary_results: Dict, save_path: str = None):
    """ì•Œê³ ë¦¬ì¦˜ë³„ ê°œì„ ìœ¨ ë¹„êµ ê·¸ë˜í”„"""
    scenarios = list(summary_results['scenarios'].keys())
    algorithms = ['dqn', 'ddqn']
    algo_labels = ['DQN', 'Double DQN']
    colors = ['#2E86AB', '#A23B72']
    
    # ê°œì„ ìœ¨ ê³„ì‚°
    improvements = {algo: [] for algo in algorithms}
    
    for scenario in scenarios:
        baseline_wait = summary_results['scenarios'][scenario]['baseline']['avg_waiting_time']
        for algo in algorithms:
            algo_wait = summary_results['scenarios'][scenario][algo]['avg_waiting_time']
            improvement = ((baseline_wait - algo_wait) / baseline_wait) * 100
            improvements[algo].append(improvement)
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, ax = plt.subplots(figsize=(12, 6))
    
    x = np.arange(len(scenarios))
    width = 0.35
    
    for i, (algo, label) in enumerate(zip(algorithms, algo_labels)):
        bars = ax.bar(x + i*width, improvements[algo], width, label=label, color=colors[i], alpha=0.8)
        # ê°’ í‘œì‹œ
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.1f}%',
                   ha='center', va='bottom', fontsize=9)
    
    ax.set_xlabel('ì‹œë‚˜ë¦¬ì˜¤', fontsize=12)
    ax.set_ylabel('ê°œì„ ìœ¨ (%)', fontsize=12)
    ax.set_title('Baseline ëŒ€ë¹„ ëŒ€ê¸°ì‹œê°„ ê°œì„ ìœ¨', fontsize=14, fontweight='bold')
    ax.set_xticks(x + width/2)
    ax.set_xticklabels([s.replace('_', ' ').title() for s in scenarios], rotation=15, ha='right')
    ax.legend(fontsize=10)
    ax.grid(True, alpha=0.3, axis='y')
    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    plt.close()


def plot_heatmap_comparison(summary_results: Dict, save_path: str = None):
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ íˆíŠ¸ë§µ"""
    scenarios = list(summary_results['scenarios'].keys())
    algorithms = ['baseline', 'dqn', 'ddqn']
    algo_labels = ['Baseline', 'DQN', 'Double DQN']
    
    # ëŒ€ê¸°ì‹œê°„ ë°ì´í„° ì¤€ë¹„
    waiting_matrix = []
    for scenario in scenarios:
        row = []
        for algo in algorithms:
            row.append(summary_results['scenarios'][scenario][algo]['avg_waiting_time'])
        waiting_matrix.append(row)
    
    waiting_matrix = np.array(waiting_matrix)
    
    # ê·¸ë˜í”„ ìƒì„±
    fig, ax = plt.subplots(figsize=(10, 8))
    
    im = ax.imshow(waiting_matrix, cmap='RdYlGn_r', aspect='auto')
    
    # ì¶• ì„¤ì •
    ax.set_xticks(np.arange(len(algo_labels)))
    ax.set_yticks(np.arange(len(scenarios)))
    ax.set_xticklabels(algo_labels)
    ax.set_yticklabels([s.replace('_', ' ').title() for s in scenarios])
    
    # ê°’ í‘œì‹œ
    for i in range(len(scenarios)):
        for j in range(len(algorithms)):
            text = ax.text(j, i, f'{waiting_matrix[i, j]:.1f}ì´ˆ',
                          ha="center", va="center", color="black", fontweight='bold')
    
    ax.set_title('ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ (í‰ê·  ëŒ€ê¸°ì‹œê°„)', fontsize=14, fontweight='bold', pad=20)
    
    # ì»¬ëŸ¬ë°”
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('í‰ê·  ëŒ€ê¸°ì‹œê°„ (ì´ˆ)', rotation=270, labelpad=20)
    
    plt.tight_layout()
    
    if save_path:
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    plt.close()


def plot_comprehensive_dashboard(summary_results: Dict, save_path: str = None):
    """ì¢…í•© ëŒ€ì‹œë³´ë“œ ê·¸ë˜í”„"""
    scenarios = list(summary_results['scenarios'].keys())
    algorithms = ['dqn', 'ddqn', 'baseline']
    algo_labels = ['DQN', 'Double DQN', 'Baseline']
    
    fig = plt.figure(figsize=(18, 12))
    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)
    
    # 1. ì‹œë‚˜ë¦¬ì˜¤ë³„ ëŒ€ê¸°ì‹œê°„ ë¹„êµ (ë°” ì°¨íŠ¸)
    ax1 = fig.add_subplot(gs[0, 0])
    x = np.arange(len(scenarios))
    width = 0.25
    colors = ['#2E86AB', '#A23B72', '#F18F01']
    
    for i, (algo, label) in enumerate(zip(algorithms, algo_labels)):
        waiting_times = [summary_results['scenarios'][s][algo]['avg_waiting_time'] for s in scenarios]
        ax1.bar(x + i*width, waiting_times, width, label=label, color=colors[i], alpha=0.8)
    
    ax1.set_xlabel('ì‹œë‚˜ë¦¬ì˜¤', fontsize=11)
    ax1.set_ylabel('í‰ê·  ëŒ€ê¸°ì‹œê°„ (ì´ˆ)', fontsize=11)
    ax1.set_title('ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  ëŒ€ê¸°ì‹œê°„ ë¹„êµ', fontsize=12, fontweight='bold')
    ax1.set_xticks(x + width)
    ax1.set_xticklabels([s.replace('_', ' ').title() for s in scenarios], rotation=15, ha='right', fontsize=9)
    ax1.legend(fontsize=9)
    ax1.grid(True, alpha=0.3, axis='y')
    
    # 2. ì‹œë‚˜ë¦¬ì˜¤ë³„ Reward ë¹„êµ
    ax2 = fig.add_subplot(gs[0, 1])
    for i, (algo, label) in enumerate(zip(algorithms, algo_labels)):
        rewards = [summary_results['scenarios'][s][algo]['avg_reward'] for s in scenarios]
        ax2.bar(x + i*width, rewards, width, label=label, color=colors[i], alpha=0.8)
    
    ax2.set_xlabel('ì‹œë‚˜ë¦¬ì˜¤', fontsize=11)
    ax2.set_ylabel('í‰ê·  Reward', fontsize=11)
    ax2.set_title('ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  Reward ë¹„êµ', fontsize=12, fontweight='bold')
    ax2.set_xticks(x + width)
    ax2.set_xticklabels([s.replace('_', ' ').title() for s in scenarios], rotation=15, ha='right', fontsize=9)
    ax2.legend(fontsize=9)
    ax2.grid(True, alpha=0.3, axis='y')
    
    # 3. ê°œì„ ìœ¨ ë¹„êµ
    ax3 = fig.add_subplot(gs[1, 0])
    improvements = {'dqn': [], 'ddqn': []}
    for scenario in scenarios:
        baseline_wait = summary_results['scenarios'][scenario]['baseline']['avg_waiting_time']
        for algo in ['dqn', 'ddqn']:
            algo_wait = summary_results['scenarios'][scenario][algo]['avg_waiting_time']
            improvement = ((baseline_wait - algo_wait) / baseline_wait) * 100
            improvements[algo].append(improvement)
    
    x_imp = np.arange(len(scenarios))
    width_imp = 0.35
    for i, (algo, label) in enumerate(zip(['dqn', 'ddqn'], ['DQN', 'Double DQN'])):
        bars = ax3.bar(x_imp + i*width_imp, improvements[algo], width_imp, 
                       label=label, color=colors[i], alpha=0.8)
        for bar in bars:
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}%', ha='center', va='bottom', fontsize=8)
    
    ax3.set_xlabel('ì‹œë‚˜ë¦¬ì˜¤', fontsize=11)
    ax3.set_ylabel('ê°œì„ ìœ¨ (%)', fontsize=11)
    ax3.set_title('Baseline ëŒ€ë¹„ ëŒ€ê¸°ì‹œê°„ ê°œì„ ìœ¨', fontsize=12, fontweight='bold')
    ax3.set_xticks(x_imp + width_imp/2)
    ax3.set_xticklabels([s.replace('_', ' ').title() for s in scenarios], rotation=15, ha='right', fontsize=9)
    ax3.legend(fontsize=9)
    ax3.grid(True, alpha=0.3, axis='y')
    ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    
    # 4. íˆíŠ¸ë§µ
    ax4 = fig.add_subplot(gs[1, 1])
    waiting_matrix = []
    for scenario in scenarios:
        row = []
        for algo in algorithms:
            row.append(summary_results['scenarios'][scenario][algo]['avg_waiting_time'])
        waiting_matrix.append(row)
    waiting_matrix = np.array(waiting_matrix)
    
    im = ax4.imshow(waiting_matrix, cmap='RdYlGn_r', aspect='auto')
    ax4.set_xticks(np.arange(len(algo_labels)))
    ax4.set_yticks(np.arange(len(scenarios)))
    ax4.set_xticklabels(algo_labels)
    ax4.set_yticklabels([s.replace('_', ' ').title() for s in scenarios], fontsize=9)
    
    for i in range(len(scenarios)):
        for j in range(len(algorithms)):
            ax4.text(j, i, f'{waiting_matrix[i, j]:.1f}',
                    ha="center", va="center", color="black", fontweight='bold', fontsize=9)
    
    ax4.set_title('ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ íˆíŠ¸ë§µ', fontsize=12, fontweight='bold')
    plt.colorbar(im, ax=ax4, label='í‰ê·  ëŒ€ê¸°ì‹œê°„ (ì´ˆ)')
    
    # 5. ì•Œê³ ë¦¬ì¦˜ë³„ í‰ê·  ì„±ëŠ¥ (ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í‰ê· )
    ax5 = fig.add_subplot(gs[2, :])
    algo_avg_waiting = {}
    algo_std_waiting = {}
    
    for algo in algorithms:
        waits = [summary_results['scenarios'][s][algo]['avg_waiting_time'] for s in scenarios]
        algo_avg_waiting[algo] = np.mean(waits)
        algo_std_waiting[algo] = np.std(waits)
    
    algo_names = [algo_labels[algorithms.index(a)] for a in algorithms]
    avg_values = [algo_avg_waiting[a] for a in algorithms]
    std_values = [algo_std_waiting[a] for a in algorithms]
    
    bars = ax5.bar(algo_names, avg_values, yerr=std_values, capsize=5, 
                   color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    
    for i, (bar, avg, std) in enumerate(zip(bars, avg_values, std_values)):
        height = bar.get_height()
        ax5.text(bar.get_x() + bar.get_width()/2., height + std + 0.5,
                f'{avg:.2f}Â±{std:.2f}ì´ˆ',
                ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    ax5.set_ylabel('í‰ê·  ëŒ€ê¸°ì‹œê°„ (ì´ˆ)', fontsize=11)
    ax5.set_title('ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤ í‰ê·  ì„±ëŠ¥ ë¹„êµ (í‰ê·  Â± í‘œì¤€í¸ì°¨)', fontsize=12, fontweight='bold')
    ax5.grid(True, alpha=0.3, axis='y')
    
    # ì „ì²´ ì œëª©
    fig.suptitle('ê°•í™”í•™ìŠµ êµí†µ ì‹ í˜¸ë“± ì œì–´ - ì¢…í•© ì„±ëŠ¥ ë¶„ì„', 
                 fontsize=16, fontweight='bold', y=0.995)
    
    if save_path:
        os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ê·¸ë˜í”„ ì €ì¥: {save_path}")
    
    plt.close()


def compare_with_baseline(
    env: TrafficEnvironment,
    scenario: str,
    num_episodes: int = 100
) -> Dict:
    """Baselineê³¼ ë¹„êµ"""
    env.set_scenario(scenario)
    controller = FixedTimeController(cycle_time=30)
    
    results = {
        'episode_rewards': [],
        'avg_waiting_times': [],
        'total_vehicles_passed': [],
        'max_queue_lengths': []
    }
    
    for episode in range(num_episodes):
        state = env.reset()
        controller.reset()
        episode_reward = 0
        
        done = False
        while not done:
            action = controller.get_action(state)
            next_state, reward, done, info = env.step(action)
            episode_reward += reward
            state = next_state
        
        results['episode_rewards'].append(episode_reward)
        results['avg_waiting_times'].append(info['avg_waiting_time'])
        results['total_vehicles_passed'].append(info['total_vehicles_passed'])
        results['max_queue_lengths'].append(info['max_queue_length'])
    
    return results


def run_integrated_experiments(
    use_tuned_params: bool = True,
    tuning_results_path: str = './results/hyperparameter_tuning_results.json',
    num_train_episodes: int = 2000,
    num_eval_episodes: int = 100
):
    """
    í†µí•© ì‹¤í—˜ ì‹¤í–‰
    
    Args:
        use_tuned_params: íŠœë‹ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš© ì—¬ë¶€
        tuning_results_path: íŠœë‹ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        num_train_episodes: í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜
        num_eval_episodes: í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜
    """
    # ë¡œê±° ì´ˆê¸°í™”
    logger = ExperimentLogger('./results/experiment_log.txt')
    
    logger.log_section("í†µí•© ì‹¤í—˜ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
    logger.log(f"í•™ìŠµ ì—í”¼ì†Œë“œ: {num_train_episodes}, í‰ê°€ ì—í”¼ì†Œë“œ: {num_eval_episodes}")
    
    print("\n" + "="*70)
    print("ğŸš€ í†µí•© ì‹¤í—˜ ì›Œí¬í”Œë¡œìš° ì‹œì‘")
    print("="*70)
    
    # 1ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ
    logger.log_section("1ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œë“œ")
    if use_tuned_params and os.path.exists(tuning_results_path):
        logger.log(f"íŠœë‹ëœ íŒŒë¼ë¯¸í„° ì‚¬ìš©: {tuning_results_path}")
        best_params = find_best_hyperparameters(tuning_results_path)
        params_source = "tuned"
    else:
        logger.log("ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©")
        best_params = load_default_hyperparameters()
        params_source = "default"
    
    logger.log_hyperparameters(best_params)
    
    # í™˜ê²½ ìƒì„±
    env = TrafficEnvironment()
    logger.log("í™˜ê²½ ìƒì„± ì™„ë£Œ")
    
    # ì‹œë‚˜ë¦¬ì˜¤ ë¦¬ìŠ¤íŠ¸
    scenarios = ['normal', 'morning_rush', 'evening_rush', 'congestion', 'night']
    logger.log(f"í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤: {', '.join(scenarios)}")
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    all_results = {
        'hyperparameters': best_params,
        'params_source': params_source,
        'scenarios': {}
    }
    
    # 2ë‹¨ê³„: ê° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‹¤í—˜
    logger.log_section("2ë‹¨ê³„: ì‹œë‚˜ë¦¬ì˜¤ë³„ ì‹¤í—˜")
    for scenario in scenarios:
        logger.log_section(f"ì‹œë‚˜ë¦¬ì˜¤: {scenario}")
        print("\n" + "="*70)
        print(f"ğŸ“ ì‹œë‚˜ë¦¬ì˜¤: {scenario}")
        print("="*70)
        
        scenario_results = {}
        
        # DQN í•™ìŠµ
        logger.log_scenario_start(scenario, "DQN", num_train_episodes)
        print("\n[1/4] DQN ì—ì´ì „íŠ¸ í•™ìŠµ (ìµœì  íŒŒë¼ë¯¸í„°)")
        dqn_agent = DQNAgent(**best_params)
        dqn_history = train_agent_with_params(
            dqn_agent, env, num_train_episodes, scenario,
            f'./models/optimized/dqn_{scenario}',
            logger=logger
        )
        scenario_results['dqn_training'] = dqn_history
        logger.log(f"DQN í•™ìŠµ ì™„ë£Œ - ìµœì¢… Reward: {np.mean(dqn_history['episode_rewards'][-100:]):.2f}, "
                  f"ìµœì¢… ëŒ€ê¸°ì‹œê°„: {np.mean(dqn_history['avg_waiting_times'][-100:]):.2f}ì´ˆ")
        
        # DQN í‰ê°€
        logger.log(f"DQN í‰ê°€ ì‹œì‘ ({num_eval_episodes} ì—í”¼ì†Œë“œ)")
        print(f"\n[2/4] DQN í‰ê°€ ({num_eval_episodes} ì—í”¼ì†Œë“œ)")
        dqn_eval = evaluate_agent_optimized(
            dqn_agent, env, num_eval_episodes, scenario
        )
        scenario_results['dqn_eval'] = dqn_eval
        logger.log_scenario_result(scenario, "DQN", dqn_eval)
        
        # Double DQN í•™ìŠµ
        logger.log_scenario_start(scenario, "Double DQN", num_train_episodes)
        print("\n[3/4] Double DQN ì—ì´ì „íŠ¸ í•™ìŠµ (ìµœì  íŒŒë¼ë¯¸í„°)")
        ddqn_agent = DoubleDQNAgent(**best_params)
        ddqn_history = train_agent_with_params(
            ddqn_agent, env, num_train_episodes, scenario,
            f'./models/optimized/ddqn_{scenario}',
            logger=logger
        )
        scenario_results['ddqn_training'] = ddqn_history
        logger.log(f"Double DQN í•™ìŠµ ì™„ë£Œ - ìµœì¢… Reward: {np.mean(ddqn_history['episode_rewards'][-100:]):.2f}, "
                  f"ìµœì¢… ëŒ€ê¸°ì‹œê°„: {np.mean(ddqn_history['avg_waiting_times'][-100:]):.2f}ì´ˆ")
        
        # Double DQN í‰ê°€
        logger.log(f"Double DQN í‰ê°€ ì‹œì‘ ({num_eval_episodes} ì—í”¼ì†Œë“œ)")
        print(f"\n[4/4] Double DQN í‰ê°€ ({num_eval_episodes} ì—í”¼ì†Œë“œ)")
        ddqn_eval = evaluate_agent_optimized(
            ddqn_agent, env, num_eval_episodes, scenario
        )
        scenario_results['ddqn_eval'] = ddqn_eval
        logger.log_scenario_result(scenario, "Double DQN", ddqn_eval)
        
        # Baseline í‰ê°€
        logger.log(f"Baseline í‰ê°€ ì‹œì‘ ({num_eval_episodes} ì—í”¼ì†Œë“œ)")
        print(f"\n[Baseline] ê³ ì • ì£¼ê¸° ì‹ í˜¸ë“± í‰ê°€")
        baseline_eval = compare_with_baseline(env, scenario, num_eval_episodes)
        scenario_results['baseline_eval'] = baseline_eval
        logger.log_scenario_result(scenario, "Baseline", baseline_eval)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "-"*70)
        print(f"ğŸ“Š {scenario} ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ìš”ì•½")
        print("-"*70)
        print(f"{'ì•Œê³ ë¦¬ì¦˜':<20} {'í‰ê·  Reward':>15} {'í‰ê·  ëŒ€ê¸°ì‹œê°„':>15}")
        print("-"*70)
        print(f"{'DQN (ìµœì í™”)':<20} {np.mean(dqn_eval['episode_rewards']):>15.2f} "
              f"{np.mean(dqn_eval['avg_waiting_times']):>15.2f}ì´ˆ")
        print(f"{'Double DQN (ìµœì í™”)':<20} {np.mean(ddqn_eval['episode_rewards']):>15.2f} "
              f"{np.mean(ddqn_eval['avg_waiting_times']):>15.2f}ì´ˆ")
        print(f"{'Baseline (ê³ ì • 30ì´ˆ)':<20} {np.mean(baseline_eval['episode_rewards']):>15.2f} "
              f"{np.mean(baseline_eval['avg_waiting_times']):>15.2f}ì´ˆ")
        print("-"*70)
        
        # ê°œì„ ìœ¨ ê³„ì‚°
        baseline_waiting = np.mean(baseline_eval['avg_waiting_times'])
        dqn_waiting = np.mean(dqn_eval['avg_waiting_times'])
        ddqn_waiting = np.mean(ddqn_eval['avg_waiting_times'])
        
        dqn_improvement = ((baseline_waiting - dqn_waiting) / baseline_waiting) * 100
        ddqn_improvement = ((baseline_waiting - ddqn_waiting) / baseline_waiting) * 100
        
        print(f"\nğŸ’¡ ëŒ€ê¸°ì‹œê°„ ê°œì„ ìœ¨:")
        print(f"   DQN: {dqn_improvement:>6.2f}%")
        print(f"   Double DQN: {ddqn_improvement:>6.2f}%")
        
        # í•™ìŠµ ê³¡ì„  ì‹œê°í™”
        logger.log("í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        print(f"\nğŸ“Š í•™ìŠµ ê³¡ì„  ê·¸ë˜í”„ ìƒì„± ì¤‘...")
        plot_training_curves(
            dqn_history,
            f'./results/plots/dqn_{scenario}_training.png'
        )
        logger.log_graph_generation(f'dqn_{scenario}_training.png')
        plot_training_curves(
            ddqn_history,
            f'./results/plots/ddqn_{scenario}_training.png'
        )
        logger.log_graph_generation(f'ddqn_{scenario}_training.png')
        
        all_results['scenarios'][scenario] = scenario_results
    
    # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥
    logger.log_section("3ë‹¨ê³„: ê²°ê³¼ ì €ì¥")
    print("\n" + "="*70)
    print("ğŸ’¾ ê²°ê³¼ ì €ì¥")
    print("="*70)
    
    # JSON ì €ì¥ (history ì œì™¸í•œ ìš”ì•½ë§Œ)
    summary_results = {
        'hyperparameters': all_results['hyperparameters'],
        'params_source': all_results['params_source'],
        'scenarios': {}
    }
    
    for scenario, data in all_results['scenarios'].items():
        summary_results['scenarios'][scenario] = {
            'dqn': {
                'avg_reward': float(np.mean(data['dqn_eval']['episode_rewards'])),
                'avg_waiting_time': float(np.mean(data['dqn_eval']['avg_waiting_times'])),
                'avg_vehicles_passed': float(np.mean(data['dqn_eval']['total_vehicles_passed']))
            },
            'ddqn': {
                'avg_reward': float(np.mean(data['ddqn_eval']['episode_rewards'])),
                'avg_waiting_time': float(np.mean(data['ddqn_eval']['avg_waiting_times'])),
                'avg_vehicles_passed': float(np.mean(data['ddqn_eval']['total_vehicles_passed']))
            },
            'baseline': {
                'avg_reward': float(np.mean(data['baseline_eval']['episode_rewards'])),
                'avg_waiting_time': float(np.mean(data['baseline_eval']['avg_waiting_times'])),
                'avg_vehicles_passed': float(np.mean(data['baseline_eval']['total_vehicles_passed']))
            }
        }
    
    results_path = './results/integrated_experiment_results.json'
    os.makedirs(os.path.dirname(results_path), exist_ok=True)
    
    with open(results_path, 'w') as f:
        json.dump(summary_results, f, indent=4)
    
    print(f"âœ… ê²°ê³¼ ì €ì¥: {results_path}")
    logger.log(f"JSON ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}")
    
    # ìµœì¢… ìš”ì•½ í…Œì´ë¸”
    print("\n" + "="*70)
    print("ğŸ¯ ìµœì¢… ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print("="*70)
    print(f"\nì‚¬ìš©ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {params_source.upper()}")
    print(f"Learning Rate: {best_params['learning_rate']}")
    print(f"Gamma: {best_params['gamma']}")
    print(f"Batch Size: {best_params['batch_size']}")
    print(f"Buffer Size: {best_params['buffer_capacity']}")
    
    print("\n" + "-"*70)
    print(f"{'ì‹œë‚˜ë¦¬ì˜¤':<20} {'ì•Œê³ ë¦¬ì¦˜':<20} {'í‰ê·  ëŒ€ê¸°ì‹œê°„':>15} {'ê°œì„ ìœ¨':>12}")
    print("-"*70)
    
    for scenario in scenarios:
        data = summary_results['scenarios'][scenario]
        baseline_wait = data['baseline']['avg_waiting_time']
        
        for algo in ['dqn', 'ddqn', 'baseline']:
            wait_time = data[algo]['avg_waiting_time']
            improvement = ((baseline_wait - wait_time) / baseline_wait * 100) if algo != 'baseline' else 0.0
            
            algo_name = {
                'dqn': 'DQN',
                'ddqn': 'Double DQN',
                'baseline': 'Baseline'
            }[algo]
            
            print(f"{scenario:<20} {algo_name:<20} {wait_time:>15.2f}ì´ˆ {improvement:>11.2f}%")
    
    print("-"*70)
    
    # ì¶”ê°€ ì‹œê°í™” ìƒì„±
    logger.log_section("4ë‹¨ê³„: ì¶”ê°€ ì‹œê°í™” ìƒì„±")
    print("\nğŸ“Š ì¶”ê°€ ì‹œê°í™” ê·¸ë˜í”„ ìƒì„± ì¤‘...")
    
    # 1. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ ë¹„êµ
    plot_scenario_comparison(
        summary_results,
        './results/plots/scenario_comparison.png'
    )
    logger.log_graph_generation('scenario_comparison.png')
    
    # 2. ì•Œê³ ë¦¬ì¦˜ë³„ ê°œì„ ìœ¨ ë¹„êµ
    plot_improvement_comparison(
        summary_results,
        './results/plots/improvement_comparison.png'
    )
    logger.log_graph_generation('improvement_comparison.png')
    
    # 3. íˆíŠ¸ë§µ ë¹„êµ
    plot_heatmap_comparison(
        summary_results,
        './results/plots/performance_heatmap.png'
    )
    logger.log_graph_generation('performance_heatmap.png')
    
    # 4. ì¢…í•© ëŒ€ì‹œë³´ë“œ
    plot_comprehensive_dashboard(
        summary_results,
        './results/plots/comprehensive_dashboard.png'
    )
    logger.log_graph_generation('comprehensive_dashboard.png')
    
    print("\nğŸ“Š ìƒì„±ëœ ê·¸ë˜í”„ íŒŒì¼:")
    print("   í•™ìŠµ ê³¡ì„ :")
    print("   - ./results/plots/dqn_*_training.png")
    print("   - ./results/plots/ddqn_*_training.png")
    print("   ì„±ëŠ¥ ë¹„êµ:")
    print("   - ./results/plots/scenario_comparison.png")
    print("   - ./results/plots/improvement_comparison.png")
    print("   - ./results/plots/performance_heatmap.png")
    print("   - ./results/plots/comprehensive_dashboard.png")
    
    # ìµœì¢… ìš”ì•½ ë¡œê·¸ ê¸°ë¡
    logger.log_final_summary(summary_results)
    
    # ì‹¤í—˜ ì™„ë£Œ ë¡œê·¸
    logger.log_completion()
    
    print("\nâœ¨ ëª¨ë“  ì‹¤í—˜ ì™„ë£Œ!")
    print(f"ğŸ“ ìƒì„¸ ë¡œê·¸: {logger.log_path}")
    print("="*70)


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='í†µí•© ì‹¤í—˜ ì›Œí¬í”Œë¡œìš°')
    parser.add_argument(
        '--use-tuned',
        action='store_true',
        help='í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ ì‚¬ìš© (ê¸°ë³¸: ê¸°ë³¸ê°’ ì‚¬ìš©)'
    )
    parser.add_argument(
        '--tuning-results',
        type=str,
        default='./results/hyperparameter_tuning_results.json',
        help='í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--train-episodes',
        type=int,
        default=2000,
        help='í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜'
    )
    parser.add_argument(
        '--eval-episodes',
        type=int,
        default=100,
        help='í‰ê°€ ì—í”¼ì†Œë“œ ìˆ˜'
    )
    
    args = parser.parse_args()
    
    run_integrated_experiments(
        use_tuned_params=args.use_tuned,
        tuning_results_path=args.tuning_results,
        num_train_episodes=args.train_episodes,
        num_eval_episodes=args.eval_episodes
    )