"""
ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ í…ŒìŠ¤íŠ¸
ê° ì„¤ì •ë‹¹ 50 ì—í”¼ì†Œë“œë§Œ í•™ìŠµí•˜ì—¬ ë¹ ë¥´ê²Œ ë™ì‘ í™•ì¸
"""

import numpy as np
import torch
from traffic_env import TrafficEnvironment
from dqn_agent import DQNAgent
import json
import os


def quick_hyperparameter_test():
    """ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("ğŸ”¬ ë¹ ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸")
    print("="*60)
    print("ê° ì„¤ì •ë‹¹ 50 ì—í”¼ì†Œë“œë§Œ í•™ìŠµ")
    
    env = TrafficEnvironment()
    env.set_scenario('normal')
    
    STATE_DIM = 7
    ACTION_DIM = 2
    NUM_EPISODES = 50
    
    results = {}
    
    # ì‹¤í—˜ A: Learning Rate
    print("\n[ì‹¤í—˜ A] Learning Rate ë¹„êµ")
    lr_results = {}
    
    for lr in [0.0001, 0.001, 0.01]:
        print(f"\n  Learning Rate = {lr}")
        
        agent = DQNAgent(
            state_dim=STATE_DIM,
            action_dim=ACTION_DIM,
            learning_rate=lr,
            gamma=0.95,
            buffer_capacity=1000,
            batch_size=32
        )
        
        rewards = []
        waiting_times = []
        
        for episode in range(NUM_EPISODES):
            state = env.reset()
            episode_reward = 0
            
            done = False
            while not done:
                action = agent.select_action(state, training=True)
                next_state, reward, done, info = env.step(action)
                agent.store_transition(state, action, reward, next_state, done)
                agent.update()
                episode_reward += reward
                state = next_state
            
            agent.decay_epsilon()
            rewards.append(episode_reward)
            waiting_times.append(info['avg_waiting_time'])
        
        # ë§ˆì§€ë§‰ 20 ì—í”¼ì†Œë“œ í‰ê· 
        avg_reward = np.mean(rewards[-20:])
        avg_waiting = np.mean(waiting_times[-20:])
        
        lr_results[f'lr_{lr}'] = {
            'avg_reward': avg_reward,
            'avg_waiting_time': avg_waiting
        }
        
        print(f"    í‰ê·  Reward: {avg_reward:.2f}")
        print(f"    í‰ê·  ëŒ€ê¸°ì‹œê°„: {avg_waiting:.2f}ì´ˆ")
    
    results['Learning_Rate'] = lr_results
    
    # ì‹¤í—˜ B: Discount Factor
    print("\n[ì‹¤í—˜ B] Discount Factor ë¹„êµ")
    gamma_results = {}
    
    for gamma in [0.90, 0.95, 0.99]:
        print(f"\n  Gamma = {gamma}")
        
        agent = DQNAgent(
            state_dim=STATE_DIM,
            action_dim=ACTION_DIM,
            learning_rate=0.001,
            gamma=gamma,
            buffer_capacity=1000,
            batch_size=32
        )
        
        rewards = []
        waiting_times = []
        
        for episode in range(NUM_EPISODES):
            state = env.reset()
            episode_reward = 0
            
            done = False
            while not done:
                action = agent.select_action(state, training=True)
                next_state, reward, done, info = env.step(action)
                agent.store_transition(state, action, reward, next_state, done)
                agent.update()
                episode_reward += reward
                state = next_state
            
            agent.decay_epsilon()
            rewards.append(episode_reward)
            waiting_times.append(info['avg_waiting_time'])
        
        avg_reward = np.mean(rewards[-20:])
        avg_waiting = np.mean(waiting_times[-20:])
        
        gamma_results[f'gamma_{gamma}'] = {
            'avg_reward': avg_reward,
            'avg_waiting_time': avg_waiting
        }
        
        print(f"    í‰ê·  Reward: {avg_reward:.2f}")
        print(f"    í‰ê·  ëŒ€ê¸°ì‹œê°„: {avg_waiting:.2f}ì´ˆ")
    
    results['Discount_Factor'] = gamma_results
    
    # ì‹¤í—˜ C: Batch Size
    print("\n[ì‹¤í—˜ C] Batch Size ë¹„êµ")
    batch_results = {}
    
    for batch_size in [16, 32, 64]:
        print(f"\n  Batch Size = {batch_size}")
        
        agent = DQNAgent(
            state_dim=STATE_DIM,
            action_dim=ACTION_DIM,
            learning_rate=0.001,
            gamma=0.95,
            buffer_capacity=1000,
            batch_size=batch_size
        )
        
        rewards = []
        waiting_times = []
        
        for episode in range(NUM_EPISODES):
            state = env.reset()
            episode_reward = 0
            
            done = False
            while not done:
                action = agent.select_action(state, training=True)
                next_state, reward, done, info = env.step(action)
                agent.store_transition(state, action, reward, next_state, done)
                agent.update()
                episode_reward += reward
                state = next_state
            
            agent.decay_epsilon()
            rewards.append(episode_reward)
            waiting_times.append(info['avg_waiting_time'])
        
        avg_reward = np.mean(rewards[-20:])
        avg_waiting = np.mean(waiting_times[-20:])
        
        batch_results[f'batch_{batch_size}'] = {
            'avg_reward': avg_reward,
            'avg_waiting_time': avg_waiting
        }
        
        print(f"    í‰ê·  Reward: {avg_reward:.2f}")
        print(f"    í‰ê·  ëŒ€ê¸°ì‹œê°„: {avg_waiting:.2f}ì´ˆ")
    
    results['Batch_Size'] = batch_results
    
    # ê²°ê³¼ ìš”ì•½
    print("\n" + "="*60)
    print("ğŸ“Š ê²°ê³¼ ìš”ì•½")
    print("="*60)
    
    for exp_name, exp_results in results.items():
        print(f"\n[{exp_name}]")
        print("-" * 60)
        for param_key, metrics in exp_results.items():
            param_label = param_key.split('_', 1)[1]
            print(f"{param_label:<15} Reward: {metrics['avg_reward']:>8.2f}  "
                  f"ëŒ€ê¸°ì‹œê°„: {metrics['avg_waiting_time']:>6.2f}ì´ˆ")
        print("-" * 60)
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs('./results', exist_ok=True)
    with open('./results/quick_hyperparameter_test.json', 'w') as f:
        json.dump(results, f, indent=4)
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ê²°ê³¼ ì €ì¥: ./results/quick_hyperparameter_test.json")
    
    # ìµœì  íŒŒë¼ë¯¸í„° ì¶”ì²œ
    print("\n" + "="*60)
    print("ğŸ¯ ì¶”ì²œ íŒŒë¼ë¯¸í„° (í…ŒìŠ¤íŠ¸ ê¸°ë°˜)")
    print("="*60)
    
    # Learning Rateì—ì„œ ìµœê³  ì„±ëŠ¥
    best_lr = max(results['Learning_Rate'].items(), 
                  key=lambda x: x[1]['avg_reward'])
    print(f"Learning Rate: {best_lr[0].split('_')[1]} "
          f"(Reward: {best_lr[1]['avg_reward']:.2f})")
    
    # Gammaì—ì„œ ìµœê³  ì„±ëŠ¥
    best_gamma = max(results['Discount_Factor'].items(), 
                     key=lambda x: x[1]['avg_reward'])
    print(f"Discount Factor: {best_gamma[0].split('_')[1]} "
          f"(Reward: {best_gamma[1]['avg_reward']:.2f})")
    
    # Batch Sizeì—ì„œ ìµœê³  ì„±ëŠ¥
    best_batch = max(results['Batch_Size'].items(), 
                     key=lambda x: x[1]['avg_reward'])
    print(f"Batch Size: {best_batch[0].split('_')[1]} "
          f"(Reward: {best_batch[1]['avg_reward']:.2f})")
    
    print("\nâš ï¸  ì£¼ì˜: 50 ì—í”¼ì†Œë“œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ì´ë¯€ë¡œ ì°¸ê³ ìš©ì…ë‹ˆë‹¤.")
    print("   ì •í™•í•œ ê²°ê³¼ëŠ” hyperparameter_tuning.py ì‹¤í–‰ í•„ìš”")
    print("="*60)


if __name__ == "__main__":
    quick_hyperparameter_test()